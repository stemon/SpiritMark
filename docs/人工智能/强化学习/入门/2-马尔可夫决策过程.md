# 🍍 马尔可夫决策过程 MDP

---

> 😭 本篇知识点实在太多，还会再重新再梳理几遍

## 1. 前言

<img src="https://gitee.com/veal98/images/raw/master/img/2.2.png" style="zoom:40%;" />

上图介绍了在强化学习里面 agent 跟 environment 之间的交互，agent 在得到环境的状态过后，它会采取行为，它会把这个采取的行为返还给环境。环境在得到 agent 的行为过后，它会进入下一个状态，把下一个状态传回 agent。

在强化学习中，agent 跟环境就是这样进行交互的，**这个交互过程是可以通过马尔可夫决策过程来表示的，所以马尔可夫决策过程是强化学习里面的一个基本框架**。在马尔可夫决策过程中，它的环境是 `fully observable` ，就是全部可以观测的。但是很多时候环境里面有些量是不可观测的，但是这个部分观测的问题也可以转换成一个 MDP 的问题。

在介绍马尔可夫决策过程 MDP 之前，先给大家梳理一下马尔可夫过程 MP、马尔可夫奖励过程 MRP。这两个过程是马尔可夫决策过程的一个基础：👇

<img src="https://gitee.com/veal98/images/raw/master/img/20201021100854.png" style="zoom: 25%;" />

## 2. 马尔可夫过程 MP

### ① 马尔可夫性质 Markov Property

⭐ 如果某一个过程满足`马尔可夫性质(Markov Property)`，就是说未来的转移跟过去是独立的，即**下一个状态的产生只和当前状态有关**：（$S_t$ 表示 t 时刻的状态）

<img src="https://gitee.com/veal98/images/raw/master/img/20201021101427.png" style="zoom:72%;" />

🚩 **马尔科夫过程是一个二元组 $<S, P>$**

**马尔可夫性质是所有马尔可夫过程的基础。**

比如说我们这里有一个 $h_t$，它包含了之前的所有状态。但是这里的转移从当前 $s_t$ 转到 $s_{t+1}$ 这个状态，它是直接就等于它之前所有的状态。

### ② 马尔可夫链 Markov Chain

首先看一看`马尔可夫链(Markov Chain)`。举个例子，这个图里面有四个状态，这四个状态从 $s_1,s_2,s_3,s_4$ 之间互相转移。

<img src="https://gitee.com/veal98/images/raw/master/img/20201021102754.png" style="zoom:50%;" />

比如说从 $s_1$ 开始：

*  $s_1$ 有 0.1 的概率继续存活在 $s_1$ 状态，
*  有 0.2 的概率转移到 $s_2$， 
*  有 0.7 的概率转移到 $s_4$ 。

如果 $s_4$ 是我们当前状态的话，

* 它有 0.3 的概率转移到 $s_2$ ，
* 有 0.2 的概率转移到 $s_3$ ，
* 有 0.5 的概率留在这里。

我们可以用 `状态转移矩阵(State Transition Matrix)` 来描述这样的**状态转移**。状态转移矩阵类似于一个 conditional probability，当我们知道当前我们在 $s_t$ 这个状态过后，到达下面所有状态的一个概率。所以它**每一行其实描述了是从一个节点到达所有其它节点的概率**：

<img src="https://gitee.com/veal98/images/raw/master/img/20201021102805.png" style="zoom: 33%;" />

下图是一个马尔可夫链的例子，我们这里有七个状态。比如说从 $s_1$ 开始到 $s_2$ ，它有 0.4 的概率，然后它有 0.6 的概率继续存活在它当前的状态。 $s_2$ 有 0.4 的概率到左边，有 0.4 的概率到 $s_3$ ，另外有 0.2 的概率存活在现在的状态：

<img src="https://gitee.com/veal98/images/raw/master/img/20201021102849.png" style="zoom:40%;" />

给定了这个状态转移的马尔可夫链后，我们可以对这个链进行采样，这样就会得到一串的**轨迹**。下面我们有三个轨迹，都是从同一个起始点开始。假设还是从 $s_3$ 这个状态开始：

<img src="https://gitee.com/veal98/images/raw/master/img/20201021103425.png" style="zoom:50%;" />

## 3. 马尔可夫奖励过程 MRP

### ① 概念

**`马尔可夫奖励过程(Markov Reward Process, MRP)` 是马尔可夫链再加上了一个奖励函数。**在 MRP 中，转移矩阵跟它的这个状态都是跟马尔可夫链一样的，多了一个`奖励函数(reward function)`。**奖励函数是一个期望**，就是说当你到达某一个状态的时候，可以获得多大的奖励。这里另外定义了一个 discount factor $\gamma$ 。

<img src="https://gitee.com/veal98/images/raw/master/img/2.7.png" style="zoom:35%;" />

🚩 **马尔科夫奖励过程是一个四元组 $<S, P, R, γ>$，即相比于 MP 加入了 Reward**

下图是我们刚才看的马尔可夫链，如果把奖励也放上去的话，就是说到达每一个状态，我们都会获得一个奖励。这里我们可以设置对应的奖励，比如说到达 $s_1$ 状态的时候，可以获得 5 的奖励，到达 $s_7$ 的时候，有 10 的奖励，其它状态没有任何奖励。因为这里状态是有限的，所以我们可以用一个向量 R 来表示这个奖励函数，**这个向量表示了每个点的奖励的大小**：

<img src="https://gitee.com/veal98/images/raw/master/img/20201021104852.png" style="zoom:40%;" />

### ② Return And Value Function

<img src="https://gitee.com/veal98/images/raw/master/img/2.9.png" style="zoom:40%;" />

* 第一个是 `horizon` ，它说明了同一个 episode 或者是整个一个轨迹的长度，它是由有限个步数决定的。
* 这里我们再定义一个 `Discounted return`。Return 说的是我们把奖励进行折扣，然后获得的这个收益。Return 可以定义为奖励的逐步叠加，这里有一个**折扣系数 $γ$，就是越往后得到的奖励，折扣得越多**。<u>这说明我们其实更希望得到现有的奖励，未来的奖励就要把它打折扣</u>。
* 当我们有了这个 return 过后，就可以正式定义**一个状态的价值**了，就是 `状态价值函数 state value function` ，简称**状态函数**。对于这个MRP，它里面的定义是**关于这个 return 的期望**， 期望就是说从这个状态开始，你有可能获得多大的价值。所以这个期望也可以看成是一个对未来可能获得奖励的它的当前价值的一个表现。就是当你进入某一个状态过后，你现在就有多大的价值。

💡 **解释一下为什么需要 `discounted factor` $γ$**：

* 有些马尔可夫过程是带环的，它并没有终结，我们想避免这个无穷的奖励。
* 我们想把这个不确定性表示出来，希望尽可能快地得到奖励，而不是在未来某一个点得到奖励。
* 如果这个奖励是有实际价值的，我们可能是更希望立刻就得到奖励，而不是后面再得到奖励。
* 在人的行为里面来说的话，大家也是想得到即时奖励。
* 在有些时候可以把这个系数设为 0。设为 0 过后，我们就只关注了它当前的奖励。我们也可以把它设为 1，设为 1 的话就是对未来并没有折扣，未来获得的奖励跟当前获得的奖励是一样的。这个系数其实可以作为强化学习 agent 的一个 hyperparameter 来进行调整，然后就会得到不同行为的 agent。

💬 举个例子，在下图这个 MRP 里面，如何计算它的价值：

<img src="https://gitee.com/veal98/images/raw/master/img/2.11.png" style="zoom:45%;" />

### ② Bellman Equation 贝尔曼等式 

这里就引出了一个问题，**当我们有了一些轨迹的实际 return，怎么计算它的价值函数**。<u>比如说我们想知道 $s_4$ 状态的价值，就是当你进入 $s_4$ 后，它的价值到底如何</u>。

#### Ⅰ 概念

**Bellman Equation 定义了当前状态跟未来状态之间的这个关系**。

<img src="https://gitee.com/veal98/images/raw/master/img/2.12.png" style="zoom:40%;" />

其中：

*  $s'$ 可以看成未来的所有状态。
*  转移 $P(s'|s)$  是指从当前状态 s 转移到未来状态 $s'$ 的概率。
*  $V(s')$ 代表的是未来某一个状态的价值。我们从当前这个位置开始，有一定的概率去到未来的所有状态，乘以未来所有状态的价值，然后再乘以一个 $\gamma$，这样就可以把未来的奖励打折扣。

**未来打了折扣的奖励加上当前立刻可以得到的奖励，就组成了这个 Bellman Equation**。Bellman Equation 的推导过程如下：

$\begin{aligned}
V(s)&=\mathbb{E}\left[G_{t} \mid s_{t}=s\right]\\
&=\mathbb{E}\left[R_{t+1}+\gamma R_{t+2}+\gamma^{2} R_{t+3}+\ldots \mid s_{t}=s\right]  \\
&=\mathbb{E}\left[R_{t+1}|s_t=s\right] +\gamma \mathbb{E}\left[R_{t+2}+\gamma R_{t+3}+\gamma^{2} R_{t+4}+\ldots \mid s_{t}=s\right]\\
&=R(s)+\gamma \mathbb{E}[G_{t+1}|s_t=s] \\
&=R(s)+\gamma \mathbb{E}[V(s_{t+1})|s_t=s]\\
&=R(s)+\gamma \sum_{s^{\prime} \in S} P\left(s^{\prime} \mid s\right) V\left(s^{\prime}\right)
\end{aligned}$

> 💡 Bellman Equation 就是当前状态与未来状态的迭代关系，**表示当前状态的值函数可以通过下个状态的值函数来计算**。Bellman Equation 因其提出者、动态规划创始人 Richard Bellman 而得名 ，也叫作“**动态规划方程**”。

**Bellman Equation 定义了状态之间的迭代关系。**假设有一个马尔可夫转移矩阵是下图这个样子：

<img src="https://gitee.com/veal98/images/raw/master/img/20201021110832.png" style="zoom:50%;" />

Bellman Equation 描述的就是当前状态到未来状态的一个转移。假设我们当前是在 $s_1$， 那么它只可能去到三个未来的状态：有 0.1 的概率留在它当前这个位置，有 0.2 的概率去到 $s_2$ 状态，有 0.7 的概率去到 $s_4$ 的状态，所以我们要把这个转移乘以它未来的状态的价值，再加上它的 immediate reward 就会得到它当前状态的价值。

#### Ⅱ 求解

对于状态比较少的 MRP，我们可以使用 解析解 来求解：

对于状态比较多的 MRP，我们可以使用 迭代的方法 来求解：

- 动态规划
- 蒙特卡罗采样
- Temporal-Difference Learning

##### 解析解

我们可以把 Bellman Equation 写成一种矩阵的形式。当前这个状态是一个向量  $[V(s_1),V(s_2),\cdots,V(s_N)]^T$。从每一行来看的话，$V$ 这个向量乘以了转移矩阵里面的某一行，再加上它当前可以得到的 reward，就会得到它当前的价值：

<img src="https://gitee.com/veal98/images/raw/master/img/2.14.png" style="zoom:40%;" />

当我们写成如下的矩阵形式后:
$$
V = R+ \gamma PV
$$

就可以直接得到一个`解析解(analytic solution)`:
$$
V=(I-\gamma P)^{-1} R
$$
我们可以通过矩阵求逆的过程把这个 V 的这个价值直接求出来。但是一个问题是这个矩阵求逆的过程的复杂度是 $O(N^3)$。所以当状态非常多的时候，比如说从十个状态到一千个状态，到一百万个状态。那么当我们有一百万个状态的时候，这个转移矩阵就会是个一百万乘以一百万的矩阵，**这样一个大矩阵的话求逆是非常困难的**，**所以这种通过解析解去解，只能对于很小量的 MRP**。

##### 蒙特卡罗采样 Monte-Carlo evaluation

**我们可以通过迭代的方法来解状态非常多的 MRP**：👇

* 动态规划
* 蒙特卡罗采样
* Temporal-Difference Learning 。这个 `Temporal-Difference Learning` 叫 `TD Leanring`，它是动态规划和蒙特卡罗的一个结合。

**蒙特卡罗采样其实就是取平均值**：比如说我们要算 $s_4$ 状态的一个价值。我们就可以从 $s_4$ 状态开始，随机产生很多轨迹。每个轨迹，我们可以算到它的这个 return ，然后直接取一个平均作为现在 $s_4$ 的价值。

<img src="https://gitee.com/veal98/images/raw/master/img/2.16.png" style="zoom:40%;" />

##### 动态规划 Dynamic Programming

**动态规划的办法就是一直去迭代它的 Bellman Equation，直到它最后收敛（也就是说当这个最后更新的状态跟上一个状态变化并不大的时候，更新就可以停止），我们就可以输出最新的 $V'(s)$ 作为它当前状态的价值。**

<img src="https://gitee.com/veal98/images/raw/master/img/2.17.png" style="zoom:35%;" />

## 4. 马尔可夫决策过程 MDP

### ① 概念

**相对于 MRP，`马尔可夫决策过程(Markov Decision Process)`多了一个 `decision`，其它的定义跟 MRP 都是类似的。**即下一个状态不仅依赖于当前的状态，也依赖于在当前状态 agent 采取的行为。价值函数也同样的依赖于当前状态和行为。

🚩 **马尔科夫决策过程是一个五元组 $<S, A, P, R, γ>$，即相比于 MRP 加入了 动作集 Action**

<img src="https://gitee.com/veal98/images/raw/master/img/2.18.png" style="zoom:43%;" />

### ② MDP 和 MP/MRP 的差异

💡 **对比一下 MDP 里面的状态转移跟 MRP 以及 MP 的差异：**

<img src="https://gitee.com/veal98/images/raw/master/img/20201021145450.png" style="zoom:35%;" />

- 马尔可夫过程 MP 的转移是直接就决定。比如当前状态是 s，那么就直接通过这个转移概率决定了下一个状态是什么。
- **但对于 MDP，它的中间多了一层行为 a** ，就是说在你当前这个状态的时候，首先要决定的是采取某一种行为，因为对于采取哪种行为你有一定的不确定性，所以你到未来的状态其实也是一个概率分布。**也就是说在当前状态跟未来状态转移过程中这里多了一层决策性，这是 MDP 跟之前的马尔可夫过程 MP 很不同的一个地方。**

### ③ Policy in MDP

<img src="https://gitee.com/veal98/images/raw/master/img/20201021145351.png" style="zoom:43%;" />

解释一下上图中的符号：

- 💡 `policy π` 表示的是在给定的 state 下，一个关于 action 的概率分布。就是说，**π 表示在一个状态 s 下，agent 接下来可能会采取的任意一个 action 的概率分布**。

  例如，在状态 s 下，agent 接下来要么向右走，要么向左走(有且只有两种情况)，可能 agent有 0.9 的概率往右走，0.1的概率往左走。 `π(a|s)` 表示的就是这种概率。

  而如果只是 π 的话，就表示所有的状态的概率形成的整个策略。因为对于每一个状态 s 都会有这样一个π(a|s) ，所有状态的 π(a|s) 就形成整体策略 π。比如，我们可以说我们的策略就是在每个状态下都随机选取一个动作，这也是一种策略。我们也可以说我们的策略就是在每个状态下一直选某一个动作。总之，**策略 π 是指所有状态都要使用这个策略，不是单独指某一个状态**。

- 💡 $P^\pi(s'|s,a)$ 其实是一个联合概率，**表示在执行策略 π 的情况下，状态从 s 转移到 s’ 的概率。**

  例如我们种一棵树，在种的过程中有以下四个状态(状态空间S)：缺水(s1)，健康(s2)，溢水(s3)，死亡(s4)。我们的动作(动作空间 A)有：浇水(a1)，不浇水(a2)。

  现在假如这棵树处于缺水状态，所以 π(a1|s1) 表示在缺水状态 s1 下，我有多大的概率采取浇水 a1 这个动作。在这里要注意的是，**采取动作了并不表示状态就会改变**，不浇水的话，树的状态可能是继续缺水，也可能是死亡。**浇水的话，树的状态也可能会继续缺水，也可能会变健康。也就是状态的转移也有一定的概率**，也就是状态转移概率 $Ps′|s$，所以 <img src="https://gitee.com/veal98/images/raw/master/img/20201021152224.png" style="zoom:50%;" /> 表示采取浇水动作 a 的概率与浇水之后树能从缺水状态 s 变到健康状态 s’ 的概率的乘积。

### ④ Value Function

#### Ⅰ State-Value Function

<img src="https://gitee.com/veal98/images/raw/master/img/20201021160532.png" style="zoom: 67%;" />

顺着 MDP 的定义，我们可以把 `状态价值函数 state-value function ` （简称 价值函数）进行一个定义，它的定义是跟 MRP 是类似的。

上图这个值函数表示**从状态 s 出发，使用策略 π 所带来的累积奖赏**。这个策略是指在一个状态下，执行所有可能行动的概率，是一个概率分布。每个不同的状态的概率分布可以不一样，因为在不同的状态下，执行不同动作的概率不一样。

#### Ⅱ Action-Value Function / Q 函数

<img src="https://gitee.com/veal98/images/raw/master/img/20201021160556.png" style="zoom:67%;" />

<img src="https://gitee.com/veal98/images/raw/master/img/20201021150025.png" style="zoom:43%;" />

这里我们另外引入了一个 `Q 函数 / 状态动作值函数 (action-value function)`。

<img src="https://gitee.com/veal98/images/raw/master/img/20201021152758.png" style="zoom:50%;" /> 表示从状态 s 出发，已经采取了动作 a 之后，再使用策略 π 所带来的累积奖赏。被称为状态动作值函数。也许有人会说，这里都已经采取了动作 a 了，怎么还说再使用策略 π 呢？🚨 **要注意这里的策略 π 并不是只针对当前状态 s 的，它是一个整体的策略，对于每个状态都有这样的策略。所以这里 π 是针对当前状态 s 下采取动作 a 之后的下一个状态以及以后的状态**。

对 Q 函数中的行为函数进行加和，就可以得到价值函数。

### ⑤ Bellman Expectation Equation

通过对状态-价值函数进行一个分解，我们就可以得到一个类似于之前 MRP 的 Bellman Equation，这里叫 `Bellman Expectation Equation`。对于 Q 函数，我们也可以做类似的分解，也可以得到对于 Q 函数的 Bellman Expectation Equation：

<img src="https://gitee.com/veal98/images/raw/master/img/20201021153714.png" style="zoom:40%;" />

#### 价值函数 v 和 Q 函数的关系

<img src="https://gitee.com/veal98/images/raw/master/img/20201021160711.png" style="zoom: 67%;" />

上图白色点表示状态，黑色点表示动作，比如在这个 s 状态下，agent 可能执行左边的动作(假设概率0.4)，也可能执行右边的动作(假设概率0.6)，那么 $v_π(s)$想表达的就是一种期望 reward。而这个 0.4 和 0.6 其实就是由策略 π 决定的。

而 $q_π(s,a)$ 表示的是执行一个具体的动作后的 reward 值。所以显然，$v_π(s)$与 $q_π(s,a)$的关 系就是：

<img src="https://gitee.com/veal98/images/raw/master/img/20201021155138.png" style="zoom:60%;" />

比如种树那个例子，假如现在的状态是缺水，那么我的动作有两个，要么浇水，要么不浇水。**在还没有执行动作(即浇水或者不浇水) 之前的值函数就是 v。如果我已经浇水了，那么就是 q。采取不浇水的动作后，就是另一个 q。**

<img src="https://gitee.com/veal98/images/raw/master/img/20201021155430.png" style="zoom:50%;" />

这个图承接了上一个图，因为执行了动作 a 后，状态就可能会发生改变，这里有可能变成左边的状态(白色圆圈)，有可能变到右边的状态(白色圆圈)。而改变状态就会产生一个 r 的即时奖励，然后到达 s’ 状态，这样又回到了$v_π(s′)$，显然这是一个递归的过程。

等式右边说的是执行动作 a 之后，有可能去到哪个状态。去到不同状态的奖赏就不一样，所以期望的奖赏就是它们对应的概率乘以下一个状态的 $v_π(s′)$，然后所有可能的情况加起来就是期望的奖赏了。

💡 所以上面两个图结合起来就是：

- **对于 v 来说**：

  <img src="https://gitee.com/veal98/images/raw/master/img/20201021155627.png" style="zoom:50%;" />

- **对于 Q 来说**：

  <img src="https://gitee.com/veal98/images/raw/master/img/20201021155659.png" style="zoom:50%;" />

### ⑥ 最优价值函数 Optimal Value Function

#### Ⅰ 概念

强化学习是通过奖励或惩罚来学习怎样选择能产生最大积累奖励的行动的算法。**为了找到最好的行动，非常有效的方式是，找到那些奖励最大的状态**，即**在我们目前的环境（environment）中首先找到最有价值的状态 states**。例如，在赛车跑道上最有价值的是终点线（这里好像就是你冲刺要达到 deadline 的前一步，这个状态肯定最有价值），这也是奖励最多的状态，因此在跑道之上的状态也比在跑道之外的状态更有价值。（其实这里面就是递归的思想，当你找到了最有价值的状态，你只需要想办法得到这个状态就好了）

<img src="https://gitee.com/veal98/images/raw/master/img/20201021204028.png" style="zoom: 80%;" />

- 最优状态价值函数 $v^∗(s)$ 指的是在所有的策略产生的状态价值函数中最大的那个函数。

- 同样的，最优状态动作值函数 $q^∗(s,a)$ 指的是在所有的策略中产生的状态动作价值函数中最大的那个函数。因为我们的目标就是要找到一个使得 reward 最大化的路径，所以也就是相当于每一步都要找到最大的。

#### Ⅱ 最优策略 Optimal Policy

如何寻找最优策略呢？

如果在一个状态 s 下，agent 可以向左走或者向右走，现在知道往右走可以得到 17 单位的 reward，也就是知道了 $q(s,a=rightmove)$ ，然后我又知道往左走可以得到 80 单位的 reward，也就是 $q(s,a=leftmove)$ 。那么不用想了，肯定是向左走的策略好呀。

所以，下图的公式表示的就是，**在 q 已经得到最大化时，我们在状态 s 下，要选择哪一个策略(动作)才能去到 $q^∗$ 那里**。所以表示成下图：

<img src="https://gitee.com/veal98/images/raw/master/img/20201021204501.png" style="zoom:40%;" />

#### Ⅲ 最优状态值函数 Bellman Optimality Equation

结合下图这两个式子：

<img src="https://gitee.com/veal98/images/raw/master/img/20201021205628.png" style="zoom:67%;" />

我们就可以得到最优的状态价值函数：

$v^∗(s) = MAX_a q^∗(s,a)$

上式被称为 `Bellman Optimality Equation`。**这个 Bellman Optimality Equation 满足的时候，是说整个 MDP 已经到达最佳的状态。**

这个方程的意思就是说：**最优状态价值函数 v = 从所有最优状态动作值函数 q 中选择最大的那个**

❓ 但是我们**怎么得到最优状态动作值函数 q 呢？**👇

#### Ⅳ 最优状态动作值函数

<img src="https://gitee.com/veal98/images/raw/master/img/20201021210840.png" style="zoom:50%;" />

从上面的方程我们可以知道，最优状态动作值函数 $q^∗(s,a)$ 由两部分组成，一部分是即时奖励，另一部分是所有能够到达状态 s’ 的最优状态值函数按状态转移概率求和。

上面说到的 $v^∗(s)$ 与 $q^∗(s,a)$ 的式子组合起来有：

<img src="https://gitee.com/veal98/images/raw/master/img/20201021211000.png" style="zoom:40%;" />

这就是我们最终想要求的  $v^∗(s)$ 与 $q^∗(s,a)$。从上面两个式子我们可以知道，想要知道 $v^∗(s)$，我们需要先知道 $v^∗(s’)$。想要知道 $q^∗(s,a)$，我们需要先知道 $q^∗(s’,a)$，这是一个递归的过程。我们现在只是知道这个形式，但是具体在代码中要怎么实现就得用迭代的方法了。比如 value iteration， Policy iteration， Q-learning，Sarsa 等等。

> 😒 求解方法待完成，不行了，学不动了

## 📚 References

- [Bilibili - 李宏毅《深度强化学习》](https://www.bilibili.com/video/BV1MW411w79n)
- [Github - LeeDeepRL - Notes](https://datawhalechina.github.io/leedeeprl-notes/)
- [CSDN - 李宏毅深度强化学习笔记 - jessie](https://blog.csdn.net/cindy_1102/article/details/87904928)
- [强化学习纲要](https://github.com/zhoubolei/introRL)
- [CSDN - 强化学习(二)：马尔科夫决策过程(Markov decision process) - Webbley](https://blog.csdn.net/liweibin1994/article/details/79079884)