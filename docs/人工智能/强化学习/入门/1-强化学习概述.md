# 🚀 强化学习基础

---

## 1. 什么是强化学习 Reinforcement Learning

<img src="https://gitee.com/veal98/images/raw/master/img/20201020170513.png" style="zoom: 45%;" />

⭐ **强化学习讨论的问题是一个 智能体 (agent) 怎么在一个 复杂不确定的环境里面 去 极大化 它能获得的 奖励 (reward)。**

示意图由两部分组成：agent 和 environment。在强化学习过程中，agent 跟 environment 一直在交互。Agent 在环境里面获取到 **state (状态)**，agent 会利用这个状态输出一个 **action (决策)**。然后这个决策会放到环境之中去，环境会通过这个 agent 采取的决策，输出下一个状态以及当前的这个决策得到的**奖励 (reward)**。<u>Agent 的目的就是为了尽可能多地从环境中获取奖励</u>。

## 2. 强化学习和监督学习

先看一下强化学习和机器学习的关系：

<img src="https://gitee.com/veal98/images/raw/master/img/20201021091905.png" style="zoom: 67%;" />

我们可以**把强化学习跟监督学习做一个对比**。举个图片分类的例子，监督学习就是说我们有一大堆标定的数据，比如车、飞机、凳子这些标定的图片，**这些图片都要满足独立同分布，就是它们之间是没有关联的一个分布**。然后我们训练一个分类器，比如说右边这个神经网络。为了分辨出这个图片是车辆还是飞机，训练过程中，我们把真实的 label 给了这个网络。当这个网络做出一个错误的预测，比如现在输入了这个汽车的图片，它预测出来是飞机。我们就会直接告诉它，你这个预测是错误的，正确的 label 应该是车。然后我们把这个错误写成一个`损失函数(loss function)`，通过 Backpropagation 来训练这个网络。

<img src="https://gitee.com/veal98/images/raw/master/img/20201020171713.png" style="zoom: 45%;" />

所以在监督学习过程中，有两个假设，

- 输入的数据，标定的数据，它都是没有关联的，尽可能没有关联。因为如果有关联的话，这个网络是不好学习的。
- 我们告诉这个 learner 正确的标签是什么，这样它可以通过正确的标签来修正自己的这个预测。

**在强化学习里面，这两点其实都不满足**。举一个 Atari Breakout 游戏的例子，这是一个打砖块的游戏，控制木板，然后把这个球反弹到上面来消除这些砖块：

<img src="https://gitee.com/veal98/images/raw/master/img/20201020201007.png" style="zoom: 40%;" />

- 在游戏过程中，大家可以发现这个 **agent 得到的观测不是独立同分布，上一帧下一帧其实有非常强的连续性**。
- 另外一点，在玩游戏的过程中，你**并没有立刻获得这个反馈**。比如你现在把这个木板往右移，那么只会使得这个球往上或者往左上去一点，你并不会得到立刻的反馈。所以强化学习这么困难的原因是没有得到很好的反馈，但是你依然希望这个 agent 在这个环境里面学习。

强化学习的训练数据就是这样一个玩游戏的过程。你从第一步开始，采取一个决策，比如说你把这个往右移，接到这个球了。第二步你又做出决策，得到的 training data 是一个玩游戏的序列。比如现在是在第三步，你把这个序列放进去，你希望这个网络可以输出一个决策，在当前的这个状态应该输出往右移或者往左移：

<img src="https://gitee.com/veal98/images/raw/master/img/20201020201812.png" style="zoom:50%;" />

这里有个问题，就是**我们没有标签来说明你现在这个动作是正确还是错误，必须等到这个游戏结束才能知道现在这个动作到底是不是对最后的输赢有帮助**。这里就面临一个 `延迟奖励(Delayed Reward)`，所以就使得训练这个网络非常困难。

🚩 **总结下强化学习和监督学习的区别**：

<img src="https://gitee.com/veal98/images/raw/master/img/20201020202907.png" style="zoom:40%;" />

- 🔸 首先强化学习输入的序列的数据并不是像 supervised learning 里面这些样本都是独立同分布的。

- 🔸 另外一点是 learner 并没有被告诉你每一步正确的行为应该是什么。Learner 不得不自己去发现哪些行为可以使得它最后得到这个奖励，只能通过不停地尝试来发现最有利的 action。

- 🔸 还有一点是 agent 获得自己能力的过程中，其实是通过不断地 **试错(trial-and-error exploration)**。Exploration 和 exploitation 是强化学习里面非常核心的一个问题。
  - **`Exploration 探索 ` 是说你会去尝试一些【新的行为】，这些新的行为有可能会使你得到更高的奖励，也有可能使你一无所有。**
  
  - **`Exploitation 利用 ` 说的是你就是就采取你【已知】的可以获得最大奖励的行为，你就重复执行这个 action 就可以了，因为你已经知道可以获得一定的奖励。**
  
    > 💡 比如说我们选择吃饭的餐馆：Environment 可理解为你可以选择的餐馆，agent 代表你，Reward 就是吃到好吃的美食：
    >
    > - `Exploitation `是去你喜欢的店吃，可以吃到味道不错的；
    >
    > - 而 `Exploration `代表尝试一家新的餐馆，这就意味着可能 reward 并不一定高
  
  因此，我们需要在 exploration 和 exploitation 之间取得一个**权衡（trade-off）**，这也是在监督学习里面没有的情况。
  
- 🔸 在强化学习过程中，没有非常强的 supervisor，只有一个 `奖励信号(reward signal)`，就是环境会在很久以后告诉你之前你采取的行为到底是不是有效的。Agent 在这个强化学习里面学习的话就非常困难，因为你没有得到即时反馈。

  当你采取一个行为过后，如果是监督学习，你就立刻可以获得一个指引，就比如说你现在做出了一个错误的决定，那么正确的决定应该是谁。而在强化学习里面，环境可能会告诉你这个行为是错误的，但是它并没有告诉你正确的行为是什么。而且更困难的是，它可能是在一两分钟过后才告诉你之前的行为到底行不行。所以这也是强化学习和监督学习不同的地方。

### Exploration and Exploitation 

在强化学习里面，`Exploration` 和` Exploitation` 是两个很核心的问题。上面已经说过了，此处只是为了目录的完整性列出来 😊

#### K-armed Bandit K-臂赌博机

与一般监督学习不同，强化学习任务的最终奖赏是在多步动作之后才能观察到，这里我们不妨先考虑比较简单的情形：**最大化单步奖赏**，即仅考虑一步操作。需注意的是，即便在这样的简化情形下，强化学习仍与监督学习有显著不同，**因为机器需通过尝试来发现各个动作产生的结果，而没有训练数据告诉机器应当做哪个动作**。

想要最大化单步奖赏需考虑两个方面：一是需知道每个动作带来的奖赏，二是要执行奖赏最大的动作。若每个动作对应的奖赏是一个确定值，那么尝试遍所有的动作便能找出奖赏最大的动作。然而，更一般的情形是，一个动作的奖赏值是来自于一个概率分布，仅通过一次尝试并不能确切地获得平均奖赏值。

🚩 实际上，单步强化学习任务对应了一个理论模型，即 ` K-臂赌博机(K-armed bandit)`。K-臂赌博机也被称为 `多臂赌博机(Multi-armed bandit) `。如下图所示：

<img src="https://gitee.com/veal98/images/raw/master/img/20201020225718.png" style="zoom: 67%;" />

K-摇臂赌博机有 K 个摇臂，赌徒在投入一个硬币后可选择按下其中一个摇臂，每个摇臂以一定的概率吐出硬币，但这个概率赌徒并不知道。赌徒的目标是通过一定的策略最大化自己的奖赏，即获得最多的硬币。

- 若仅为获知每个摇臂的期望奖赏，则可采用 `仅探索(exploration-only)法` ：将所有的尝试机会平均分配给每个摇臂(即轮流按下每个摇臂)，最后以每个摇臂各自的平均吐币概率作为其奖赏期望的近似估计。
- 若仅为执行奖赏最大的动作，则可采用 `仅利用(exploitation-only)法`：按下目前最优的(即到目前为止平均奖赏最大的)摇臂，若有多个摇臂同为最优，则从中随机选取一个。

显然，仅探索法能很好地估计每个摇臂的奖赏，却会失去很多选择最优摇臂的机会；仅利用法则相反，它没有很好地估计摇臂期望奖赏，很可能经常选不到最优摇臂。因此，这两种方法都难以使最终的累积奖赏最大化。

⭐ **事实上，探索 (即估计摇臂的优劣) 和 利用 (即选择当前最优摇臂) 这两者是矛盾的，因为尝试次数(即总投币数)有限，加强了一方则会自然削弱另一方，这就是强化学习所面临的`探索-利用窘境(Exploration-Exploitation dilemma)`。显然，想要累积奖赏最大，则必须在探索与利用之间达成较好的折中**。

## 3. 强化学习的特征

通过跟监督学习比较，我们可以总结出强化学习的一些特征：

<img src="https://gitee.com/veal98/images/raw/master/img/20201020203134.png" style="zoom:40%;" />

- 首先它是有 `trial-and-error exploration`，它需要通过探索环境来获取对这个环境的理解。
- 第二点是强化学习 agent 会从环境里面获得 **延迟奖励** `Delayed reward`。
- 第三点是**在强化学习的训练过程中，时间非常重要**。因为你得到的数据都是有这个时间关联的，而不是独立同分布的。<u>在机器学习中，如果观测数据有非常强的关联，其实会使得这个训练非常不稳定。这也是为什么在监督学习中，我们希望 data 尽量是独立同分布了，这样就可以消除数据之间的相关性</u>。
- 第四点是 **agent 的行为会影响它随后得到的数据**，这一点是非常重要的。在我们训练 agent 的过程中，很多时候我们也是通过正在学习的这个 agent 去跟环境交互来得到数据。所以如果在训练过程中，这个 agent 的模型很快死掉了，那会使得我们采集到的数据是非常糟糕的，这样整个训练过程就失败了。所以在强化学习里面一个非常重要的问题就是<u>怎么让这个 agent 的行为一直稳定地提升</u>。

## 4. Why Reinforcement Learning

为什么我们关注这个强化学习，其中非常重要的一点就是**强化学习得到的这个模型可以取得超人类的结果**。监督学习获取的这些监督数据，其实是让人来标定的。比如说 ImageNet，这些图片都是人类标定的。那么我们就可以确定这个算法的 upper bound (上限)，**人类的这个标定结果决定了它永远不可能超越人类**。但是对于强化学习，它在环境里面自己探索，有非常大的潜力，它可以获得超越人的能力的这个表现，比如说 AlphaGo。

💬 这里给大家举一些在现实生活中强化学习的例子：

- 国际象棋是一个强化学习的过程，因为这个棋手就是在做出一个选择来跟对方对战。
- 在自然界中，羚羊其实也是在做一个强化学习，它刚刚出生的时候，可能都不知道怎么站立，然后它通过 `trial- and-error` 的一个尝试，三十分钟过后，它就可以跑到每小时 36 公里，很快地适应了这个环境。
- 你也可以把股票交易看成一个强化学习的问题，怎么去买卖来使你的收益极大化。

## 5. 深度强化学习 Deep Reinforcement Learning

强化学习是有一定的历史的，只是大家**把强化学习跟深度学习结合起来**，就形成了 **深度强化学习(Deep Reinforcemet Learning)**：

<img src="https://gitee.com/veal98/images/raw/master/img/20201020212120.png" style="zoom:40%;" />

- 之前的强化学习，它其实是先设计特征，然后训练代价函数的一个过程，就是它先设计了很多手工的特征，这个手工特征可以描述现在整个状态。得到这些特征过后，它就可以通过训练一个分类网络或者分别训练一个代价函数来做出决策。
- 现在我们有了深度学习，有了神经网络，那么大家也把这个过程改进成一个 **端到端的训练 end-to-end training** 的过程。**你直接输入这个状态，我们不需要去手工地设计这个特征，就可以让它直接输出 action**。那么就可以用一个神经网络来拟合我们这里的 value function 或 policy network，省去 了 特征工程 feature engineering 的过程。

## 6. 序列决策过程 Sequential Decision Making

下面我们简单的介绍下序列决策过程 👇

🚩 **序列决策过程的目标就是: 通过选择一系列的动作来 maximise total future reward**

在跟环境的交互过程中，agent 会获得很多观测。在每一个观测会采取一个动作，它也会得到一个奖励。**所以 `历史 History` 是 `观测 Observations、行为 Avtion、奖励 Reward` 的序列：**

$H_{t}=O_{1}, R_{1}, A_{1}, \ldots, A_{t-1}, O_{t}, R_{t}$

Agent 在采取当前动作的时候会依赖于它之前得到的这个历史，**所以你可以把整个游戏的状态看成关于这个历史的函数：**$S_{t}=f\left(H_{t}\right)$

> ❓ **状态和观测有什么关系？**
>
> 💡 `状态(state)` 是对世界的完整描述，不会隐藏世界的信息。`观测(observation)` 是对状态的部分描述，可能会遗漏一些信息。
>
> 在 deep RL 中，我们几乎总是用一个实值的向量、矩阵或者更高阶的张量来表示状态和观测。举个例子，<u>我们可以用 RGB 像素值的矩阵来表示一个视觉的观测，我们可以用机器人关节的角度和速度来表示一个机器人的状态</u>。

在 agent 的内部也有一个函数来更新这个状态。当 agent 的状态跟环境的状态等价的时候，我们就说这个环境是 `full observability`，就是全部可以观测。换句话说，<u>当 agent 能够观察到环境的所有状态时，我们称这个环境是 **完全可观测的（fully observed）**</u>：

<img src="https://gitee.com/veal98/images/raw/master/img/20201020214641.png" style="zoom:40%;" />

但是有一种情况是 agent 得到的观测并不能包含所有环境运作的状态，因为在这个强化学习的设定里面，环境的状态才是真正的所有状态。比如 agent 在玩这个 black jack 这个游戏，它能看到的其实是牌面上的牌。或者在玩雅达利游戏的时候，观测到的只是当前电视上面这一帧的信息，你并没有得到游戏内部里面所有的运作状态。也就是说当 agent 只能看到部分的观测，我们就称这个环境是 **部分可观测的（partially observed）**。在这种情况下面，强化学习通常被建模成一个 `POMDP`（**部分可观测马尔可夫决策过程 Partially Observable Markov Decision Processes**） 的问题。

> 🔊 【马尔可夫决策过程 MDP】 见下一章 

POMDP 是一个马尔可夫决策过程的泛化。POMDP 依然具有马尔可夫性质，但是假设智能体无法感知环境的状态 ，只能知道部分观测值。比如在自动驾驶中，智能体只能感知传感器采集的有限的环境信息。

POMDP 可以用一个 7 元组描述：(S,A,T,R,Ω,O,γ)，其中 S 表示状态空间，为隐变量，A 为动作空间，$T(s'|s,a)$ 为状态转移概率，R 为奖励函数，$\Omega(o|s,a)$ 为观测概率，O为观测空间，γ 为折扣系数。

### Learning and Planning

<u>**学习 Learning** 和 **规划 Planning** 是序列决策的两个基本问题</u>：

- 🔹 在 reinforcement learning 中，环境初始时是未知的，agent 不知道环境如何工作，agent 通过不断地与环境交互，逐渐改进策略。

  <img src="https://gitee.com/veal98/images/raw/master/img/20201020224922.png" style="zoom:30%;" />

- 🔹 在 plannning 中，环境是已知的，我们被告知了整个环境的运作规则的详细信息。Agent 能够计算出一个完美的模型，并且在不需要与环境进行任何交互的时候进行计算。Agent 不需要实时地与环境交互就能知道未来环境，只需要知道当前的状态，就能够开始思考，来寻找最优解。

  <img src="https://gitee.com/veal98/images/raw/master/img/20201020224955.png" style="zoom:33%;" />

  在这个游戏中，规则是制定的，我们知道选择 left 之后环境将会产生什么变化。我们完全可以通过已知的变化规则，来在内部进行模拟整个决策过程，无需与环境交互。

💡 **一个常用的强化学习问题解决思路是，先学习环境如何工作，也就是了解环境工作的方式，即学习得到一个模型，然后利用这个模型进行规划。**

## 7. 动作空间 Action Space

不同的环境允许不同种类的动作，在给定的环境中，有效动作的集合经常被称为 **动作空间(action space)**：

- 像 Atari 和 Go 这样的环境有 **离散动作空间 discrete action spaces**，在这个动作空间里，agent 的动作数量是有限的。

  例如：走迷宫机器人如果只有东南西北这 4 种移动方式，则其为离散动作空间；

- 在其他环境，比如在物理世界中控制一个 agent，在这个环境中就有 **连续动作空间 continuous action spaces**。在连续空间中，动作是实值的向量。

  例如：如果机器人向 360 中的任意角度都可以移动，则为连续动作空间。

## 8. Agent 的主要组成成分

⭐ 对于一个强化学习 agent，它有如下组成成分：

- 首先 agent 有一个 `策略函数 policy function`，agent 会用这个函数来选取下一步的动作。
- 然后它也可能生成一个`价值函数(value function)`。我们用价值函数来对当前状态进行估价，它就是说你进入现在这个状态，可以对你后面的收益带来多大的影响。当这个价值函数大的时候，说明你进入这个状态越有利。
- 另外一个组成成分是`模型(model)`。模型表示了 agent 对这个环境的状态进行了理解，它决定了这个世界是如何进行的。

当我们有了这三个成分过后，就形成了一个 `马尔可夫决策过程 keMarkov Decision Process`。这个决策过程可视化了状态之间的转移以及采取的行为：

<img src="https://gitee.com/veal98/images/raw/master/img/20201020220902.png" style="zoom:35%;" />

我们深入看这三个组成成分的一些细节 👇

### ① Policy

**Policy 决定了这个 agent 的行为，它其实是一个函数，把输入的状态变成行为**。这里有两种 policy：

- 一种是 `stochastic policy(随机性策略)`，它就是 $\pi$ 函数 $[\pi(a | s)=P\left[A_{t}=a | S_{t}=s\right]]$  。当你输入一个状态 s 的时候，输出是一个概率。这个概率就是你所有行为的一个概率，然后你可以进一步对这个概率分布进行采样，得到真实的你采取的行为。比如说这个概率可能是有 70% 的概率往左，30% 的概率往右，那么你通过采样就可以得到一个 action。
- 一种是 `deterministic policy(确定性策略)`，就是说你这里有可能只是采取它的极大化，采取最有可能的动作。你现在这个概率就是事先决定好的。

从 Atari 游戏来看的话，policy function 的输入就是游戏的一帧，它的输出决定你是往左走或者是往右走。

通常情况下，强化学习一般使用`随机性策略`。随机性策略有很多优点：

- 在学习时可以通过引入一定随机性来更好地探索环境；
- 随机性策略的动作具有多样性，这一点在多个智能体博弈时也非常重要。采用确定性策略的智能体总是对同样的环境做出相同的动作，会导致它的策略很容易被对手预测。

### ② Value Function

**价值函数是未来奖励的一个预测，用来评估状态的好坏**。

<img src="https://gitee.com/veal98/images/raw/master/img/20201020220510.png" style="zoom:35%;" />

价值函数里面有一个 **折扣系数 $γ$（ discount factor ）**，我们希望尽可能在短的时间里面得到尽可能多的奖励。如果我们说十天过后，我给你 100 块钱，跟我现在给你 100 块钱，你肯定更希望我现在就给你 100 块钱，因为你可以把这 100 块钱存在银行里面，你就会有一些利息。所以我们就通过把这个 折扣系数 $γ$ 放到价值函数的定义里面，价值函数的定义其实是一个期望。这里有一个期望 $\mathbb{E}_{\pi}$，这里有个小角标是 π 函数，这个 π 函数就是说在我们已知某一个 policy function 的时候，到底可以得到多少的奖励。

我们还有一种价值函数：**Q 函数**。Q 函数里面包含两个变量：状态 s 和动作 a 。所以你未来可以获得多少的奖励，它的这个期望取决于你当前的状态和当前的行为。这个 Q 函数是强化学习算法里面要学习的一个函数。因为当我们得到这个 Q 函数后，进入某一种状态，它最优的行为就可以通过这个 Q 函数来得到。

### ③ Model

**模型决定了下一个状态会是什么样的，就是说下一步的状态取决于你当前的状态以及你当前采取的行为。**它由两个部分组成，

- 一个是 probability，它这个转移状态之间是怎么转移的。
- 另外是这个奖励函数，当你在当前状态采取了某一个行为，可以得到多大的奖励。

<img src="https://gitee.com/veal98/images/raw/master/img/20201020220615.png" style="zoom:40%;" />

## 9. Agent 分类

### ① 基于以 Policy 为中心还是以 Value 为中心

**根据强化学习 agent 的不同，我们可以把 agent 进行归类：**

- **基于价值函数的 agent（`Value-based agent`）。**这一类 agent 显式地学习的是价值函数，隐式地学习了它的策略。因为这个策略是从我们学到的价值函数里面推算出来的。
- **基于策略的 agent（`Policy-based agent`）**，它直接去学习 policy，就是说你直接给它一个 state，它就会输出这个动作的概率。在这个 policy-based agent 里面并没有去学习它的价值函数。
- **把 value-based 和 policy-based 结合起来就有了 `Actor-Critic agent`。**这一类 agent 就把它的策略函数和价值函数都学习了，然后通过两者的交互得到一个最佳的行为。也就是说，Agent 会根据策略做出动作，而价值函数会对做出的动作给出价值，这样可以在原有的策略梯度算法的基础上加速学习过程，取得更好的效果

💬 这里我们来看一个走迷宫的例子，这个例子要求 agent 从 start 开始，然后到达 goal 的位置。这里设定的奖励是每走一步，你就会得到一个负的奖励。这里可以采取的动作是往上下左右走。当前状态用现在 agent 所在的位置来描述。

我们可以用不同的强化学习算法来解这个环境，如果我们采取的是 `基于策略的强化学习 Policy-based RL`，当我们学习好了这个环境过后，在每一个状态，我们就会得到一个最佳的行为。比如说现在在第一格开始的时候，我们知道它最佳行为是往右走，然后第二格的时候，得到的最佳策略是往上走，第三格是往右走。通过这个最佳的策略，我们就可以最快地到达终点：

<img src="https://gitee.com/veal98/images/raw/master/img/20201020221200.png" style="zoom:40%;" />

如果换成 `value-based RL` 这个算法，利用价值函数来作为导向，我们就会得到另外一种表征：

<img src="https://gitee.com/veal98/images/raw/master/img/20201020221330.png" style="zoom: 40%;" />

这里就表征了你每一个状态会返回一个价值，比如说你在 start 位置的时候，价值是 -16，因为你最快可以 16 步到达终点。因为每走一步会减一，所以你这里的价值是 -16。当我们快接近最后终点的时候，这个数字变得越来越大。在拐角的时候，比如要现在在第二格 -15。然后 agent 会看上下，它看到上面值变大了，变成 -14 了，它下面是 -16，那么这个 agent 肯定就会采取一个往上走的策略。所以通过这个学习的值的不同，我们可以抽取出现在最佳的策略。

<br>

❓ **基于策略迭代和基于价值迭代的强化学习方法有什么区别？**

在`基于策略迭代`的强化学习方法中，智能体会 **制定一套动作策略**（确定在给定状态下需要采取何种动作），并根据这个策略进行操作。强化学习算法直接对策略进行优化，使制定的策略能够获得最大的奖励。

而在`基于价值迭代`的强化学习方法中，智能体不需要制定显式的策略，它 **维护一个价值表格或价值函数**，并通过这个价值表格或价值函数来选取价值最大的动作。基于价值迭代的方法只能应用在不连续的、离散的环境下（如围棋或某些游戏领域），对于行为集合规模庞大、动作连续的场景（如机器人控制领域），其很难学习到较好的结果（此时基于策略迭代的方法能够根据设定的策略来选择连续的动作)。

### ② 基于是否需要对真实环境建模

**针对是否需要对真实环境建模，强化学习可以分为有模型学习和免模型学习：**

- 第一种是 `model-based(有模型)` RL agent，它通过学习这个状态的转移来采取措施。

  通俗来说：<u>有模型学习是指根据环境中的经验，构建一个虚拟世界，同时在真实环境和虚拟世界中学习</u>

- 另外一种是 ` model-free(免模型)` RL agent，它没有去直接估计这个状态的转移，也没有得到环境的具体转移变量。它通过学习 value function 和 policy function 进行决策。这种 model-free 的模型里面没有一个环境转移的一个模型。

  通俗来说：<u>免模型学习是指不对环境进行建模，直接与真实环境进行交互来学习到最优策略</u>。

我们可以用马尔可夫决策过程来定义强化学习任务，并表示为四元组 <S,A,P,R>，即状态集合、动作集合、状态转移函数和奖励函数。如果这四元组中所有元素均已知，且状态集合和动作集合在有限步数内是有限集，则机器可以对真实环境进行建模，构建一个虚拟世界来模拟真实环境的状态和交互反应。

具体来说，**当智能体知道状态转移函数 $P(s_{t+1}|s_t,a_t)$ 和奖励函数 $R(s_t,a_t)$ 后，它就能知道在某一状态下执行某一动作后能带来的奖励和环境的下一状态，这样智能体就不需要在真实环境中采取动作，直接在虚拟世界中学习和规划策略即可**。这种学习方法称为`有模型学习`。

<img src="https://gitee.com/veal98/images/raw/master/img/20201020222759.png" style="zoom:50%;" />

然而在实际应用中，智能体并不是那么容易就能知晓 MDP 中的所有元素的。<u>通常情况下，状态转移函数和奖励函数很难估计，甚至连环境中的状态都可能是未知的，这时就需要采用免模型学习</u>。**免模型学习没有对真实环境进行建模，智能体只能在真实环境中通过一定的策略来执行动作，等待奖励和状态迁移，然后根据这些反馈信息来更新行为策略，这样反复迭代直到学习到最优策略。**

⭐ **总的来说，有模型学习相比于免模型学习仅仅多出一个步骤，即对真实环境进行建模**。因此，一些有模型的强化学习方法，也可以在免模型的强化学习方法中使用。在实际应用中，如果不清楚该用有模型强化学习还是免模型强化学习，可以先思考一下，<u>在智能体执行动作前，是否能对下一步的状态和奖励进行预测，如果可以，就能够对环境进行建模，从而采用有模型学习</u>。

免模型学习通常属于数据驱动型方法，需要大量的采样来估计状态、动作及奖励函数，从而优化动作策略。例如，在 Atari 平台上的 Space Invader 游戏中，免模型的深度强化学习需要大约 2 亿帧游戏画面才能学到比较理想的效果。相比之下，有模型学习可以在一定程度上缓解训练数据匮乏的问题，因为智能体可以在虚拟世界中行训练。

**免模型学习的泛化性要优于有模型学习，原因是有模型学习算需要对真实环境进行建模，并且虚拟世界与真实环境之间可能还有差异，这限制了有模型学习算法的泛化性。**

有模型的强化学习方法可以对环境建模，使得该类方法具有独特魅力，即“想象能力”。在免模型学习中，智能体只能一步一步地采取策略，等待真实环境的反馈；而有模型学习可以在虚拟世界中预测出所有将要发生的事，并采取对自己最有利的策略。

目前，大部分深度强化学习方法都采用了免模型学习，这是因为：

- 免模型学习更为简单直观且有丰富的开源资料，像 DQN、AlphaGo 系列等都采用免模型学习；
- 在目前的强化学习研究中，大部分情况下环境都是静态的、可描述的，智能体的状态是离散的、可观察的（如 Atari 游戏平台），<u>这种相对简单确定的问题并不需要评估状态转移函数和奖励函数，直接采用免模型学习，使用大量的样本进行训练就能获得较好的效果</u>。

## 📚 References

- [Bilibili - 李宏毅《深度强化学习》](https://www.bilibili.com/video/BV1MW411w79n)
- [Github - LeeDeepRL - Notes](https://datawhalechina.github.io/leedeeprl-notes/)
- [CSDN - 李宏毅深度强化学习笔记 - jessie](https://blog.csdn.net/cindy_1102/article/details/87904928)
- [强化学习纲要](https://github.com/zhoubolei/introRL)